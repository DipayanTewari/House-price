{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\n\nimport scikitplot\nimport seaborn as sns\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.utils import np_utils","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')\nprint(df.shape)\ndf.head()","execution_count":3,"outputs":[{"output_type":"stream","text":"(35887, 3)\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   emotion                                             pixels     Usage\n0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emotion</th>\n      <th>pixels</th>\n      <th>Usage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n      <td>Training</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n      <td>Training</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.emotion.unique()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"array([0, 2, 4, 6, 3, 5, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"emotion_label_to_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 4: 'sadness', 5: 'surprise', 6: 'neutral'}","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.emotion.value_counts()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"3    8989\n6    6198\n4    6077\n2    5121\n0    4953\n5    4002\n1     547\nName: emotion, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df.emotion)\npyplot.show()","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"`So majority classes belongs to 3:Happy, 4:Sad and 6:Neutral nd we are also intersted in these three classes only.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"math.sqrt(len(df.pixels[0].split(' ')))","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"48.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = pyplot.figure(1, (14, 14))\n\nk = 0\nfor label in sorted(df.emotion.unique()):\n    for j in range(7):\n        px = df[df.emotion==label].pixels.iloc[k]\n        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')\n\n        k += 1\n        ax = pyplot.subplot(7, 7, k)\n        ax.imshow(px, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(emotion_label_to_text[label])\n        pyplot.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INTERESTED_LABELS = [3, 4, 6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df.emotion.isin(INTERESTED_LABELS)]\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Now I will make the data compatible for neural networks.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))\nimg_array = np.stack(img_array, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nimg_labels = le.fit_transform(df.emotion)\nimg_labels = np_utils.to_categorical(img_labels)\nimg_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Splitting the data into training and validation set.`"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(img_array, img_labels,\n                                                    shuffle=True, stratify=img_labels,\n                                                    test_size=0.1, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df\ndel img_array\ndel img_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalizing results, as neural networks are very sensitive to unnormalized data.\nX_train = X_train / 255.\nX_valid = X_valid / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_net(optim):\n    \"\"\"\n    This is a Deep Convolutional Neural Network (DCNN). For generalization purpose I used dropouts in regular intervals.\n    I used `ELU` as the activation because it avoids dying relu problem but also performed well as compared to LeakyRelu\n    atleast in this case. `he_normal` kernel initializer is used as it suits ELU. BatchNormalization is also used for better\n    results.\n    \"\"\"\n    net = Sequential(name='DCNN')\n\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(5,5),\n            input_shape=(img_width, img_height, img_depth),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_1'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_1'))\n    net.add(\n        Conv2D(\n            filters=64,\n            kernel_size=(5,5),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_2'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_2'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n    net.add(Dropout(0.4, name='dropout_1'))\n\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_3'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_3'))\n    net.add(\n        Conv2D(\n            filters=128,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_4'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_4'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n    net.add(Dropout(0.4, name='dropout_2'))\n\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_5'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_5'))\n    net.add(\n        Conv2D(\n            filters=256,\n            kernel_size=(3,3),\n            activation='elu',\n            padding='same',\n            kernel_initializer='he_normal',\n            name='conv2d_6'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_6'))\n    \n    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))\n    net.add(Dropout(0.5, name='dropout_3'))\n\n    net.add(Flatten(name='flatten'))\n        \n    net.add(\n        Dense(\n            128,\n            activation='elu',\n            kernel_initializer='he_normal',\n            name='dense_1'\n        )\n    )\n    net.add(BatchNormalization(name='batchnorm_7'))\n    \n    net.add(Dropout(0.6, name='dropout_4'))\n    \n    net.add(\n        Dense(\n            num_classes,\n            activation='softmax',\n            name='out_layer'\n        )\n    )\n    \n    net.compile(\n        loss='categorical_crossentropy',\n        optimizer=optim,\n        metrics=['accuracy']\n    )\n    \n    net.summary()\n    \n    return net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nI used two callbacks one is `early stopping` for avoiding overfitting training data\nand other `ReduceLROnPlateau` for learning rate.\n\"\"\"\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.00005,\n    patience=11,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    factor=0.5,\n    patience=7,\n    min_lr=1e-7,\n    verbose=1,\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the data in hand is less as compared to the task so ImageDataGenerator is good to go.\ntrain_datagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    zoom_range=0.15,\n    horizontal_flip=True,\n)\ntrain_datagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32 #batch size of 32 performs the best.\nepochs = 100\noptims = [\n    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),\n    optimizers.Adam(0.001),\n]\n\n# I tried both `Nadam` and `Adam`, the difference in results is not different but I finally went with Nadam as it is more popular.\nmodel = build_net(optims[1]) \nhistory = model.fit_generator(\n    train_datagen.flow(X_train, y_train, batch_size=batch_size),\n    validation_data=(X_valid, y_valid),\n    steps_per_epoch=len(X_train) / batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n    \nmodel.save(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\nfig = pyplot.figure(0, (12, 4))\n\nax = pyplot.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('epoch_history_dcnn.png')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    The epochs history shows that accuracy gradually increases and achieved +83% accuracy on both training and validation set, but at the end the model starts overfitting training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n\nfig = pyplot.figure(0, (14, 4))\nax = pyplot.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\npyplot.title('Accuracy')\npyplot.tight_layout()\n\nax = pyplot.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\npyplot.title('Loss')\npyplot.tight_layout()\n\npyplot.savefig('performance_dist.png')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat_valid = model.predict_classes(X_valid)\nscikitplot.metrics.plot_confusion_matrix(np.argmax(y_valid, axis=1), yhat_valid, figsize=(7,7))\npyplot.savefig(\"confusion_matrix_dcnn.png\")\n\nprint(f'total wrong validation predictions: {np.sum(np.argmax(y_valid, axis=1) != yhat_valid)}\\n\\n')\nprint(classification_report(np.argmax(y_valid, axis=1), yhat_valid))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix clearly shows that our model is doing good job on the class `happy` but it's performance is low on other two classes. One of the reason for this could be the fact that these two classes have less data. But when I looked at the images I found some images from these two classes are even hard for a human to tell whether the person is sad or neutral. Facial expression depends on individual as well. Some person's neutral face looks like sad."},{"metadata":{"trusted":true},"cell_type":"code","source":"mapper = {\n    0: \"happy\",\n    1: \"sad\",\n    2: \"neutral\",\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(2)\nrandom_sad_imgs = np.random.choice(np.where(y_valid[:, 1]==1)[0], size=9)\nrandom_neutral_imgs = np.random.choice(np.where(y_valid[:, 2]==1)[0], size=9)\n\nfig = pyplot.figure(1, (18, 4))\n\nfor i, (sadidx, neuidx) in enumerate(zip(random_sad_imgs, random_neutral_imgs)):\n        ax = pyplot.subplot(2, 9, i+1)\n        sample_img = X_valid[sadidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"true:sad, pred:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        ax = pyplot.subplot(2, 9, i+10)\n        sample_img = X_valid[neuidx,:,:,0]\n        ax.imshow(sample_img, cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"t:neut, p:{mapper[model.predict_classes(sample_img.reshape(1,48,48,1))[0]]}\")\n\n        pyplot.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See in the first row 7th image looks more like neutral rather than sad and our model even predicted it neutral. Whereas the last image in second row is very much sad."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}